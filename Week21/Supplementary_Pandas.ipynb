{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Pandas Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we provide extra material on the package `pandas`. As this is an introductory course, we only provide a brief introduction to `pandas` and a flavour of why it may be useful to you. For more information see the [`pandas` documentation here](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n",
    "The strength of `pandas` lies in it's ability to handle database and spreadsheet type operations that may be more familiar to users of languages such as `R` and `SQL`. \n",
    "\n",
    " > **Note:** By convention, `pandas` is often aliased (renamed when importing) as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Pandas datatypes](#Pandas-datatypes)\n",
    " - [Pandas Series](#Pandas-Series)\n",
    " - [Pandas Dataframes](#Pandas-Dataframes)\n",
    "- [Getting indices](#Getting-indices)\n",
    "- [Working with Pandas](#Working-with-Pandas)\n",
    " - [Boolean indexing](#Boolean-indexing)\n",
    " - [Querying](#Querying)\n",
    " - [Removing NaNs](#Removing-NaNs)\n",
    " - [Sorting data](#Sorting-data)\n",
    " - [Summarizing by column](#Summarizing-by-column)\n",
    " - [Summarizing by groupings](#Summarizing-by-groupings)\n",
    " - [Reshaping](#Reshaping)\n",
    "- [Reading/Writing files](#Reading/Writing-files)\n",
    "- [Using pandas](#Using-pandas)\n",
    "- [Exercises](#Exercises) (Recommended)\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas datatypes\n",
    "\n",
    "The world of `pandas` revolves predominantly around the `Series` and `DataFrame` datatypes. We shall look at each one of these in turn now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `Series`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `pandas` `Series` object is an indexed one-dimensional array of data.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_series = pd.Series([2.1,3.9,4.2])\n",
    "print(example_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in a `pandas` `Series` can be converted to a `numpy` `array` using the `values` attribute like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of a `Series` can also be accessed using square brackets in the same way elements in a one-dimensional `numpy` array are accessed. For example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series[0])\n",
    "print(example_series[0:2])\n",
    "print(example_series[example_series>3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key features of a `Series` object is that you can also change how data is indexed. For example, when creating a `Series` alternate indices can be specified using the `index` argument like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_series = pd.Series([2.1,3.9,4.2],\n",
    "                           index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices specified above can now be used to access the data, much like using keys to access a Python `dict`, i.e. using square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that numerical indices can still be used like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `numpy` style array indexing syntax can be used on custom indices. For example, the colon operator, `:`, can still be used for indexing, but on non-numerical indices, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series['a':'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note**: This indexing, unlike `numpy`, is inclusive. For example, `'a':'b'` includes element `'b'`, unlike in `numpy` where `0:2` does not include element `2`.\n",
    " \n",
    " > **Note:** If a number is used as a custom index for a column; the custom indices take precedence over the inbuilt `numpy` like indexing. For example:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have tried to label the first element in the Series as 2\n",
    "example_series = pd.Series([2.1,3.9,4.2],\n",
    "                           index=[2, 'b', 'c'])\n",
    "\n",
    "# Lets see whether we get the first element like we specified\n",
    "# or if we get the 3rd element like in normal python zero indexing\n",
    "print(example_series[2])\n",
    "# Spoiler Alert: We got the first element!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `pandas` `Series` can also be constructed from a `dict`. When a `Series` is constructed using a `dict`, the keys in the `dict` become the indices of the `Series` and the values in the `dict` become the values in the `Series`. For example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple dict with 3 key-value pairs\n",
    "example_dict = {'a': 1, 'b': 33, 'c': 2}\n",
    "print(example_dict)\n",
    "\n",
    "# Converted to a series\n",
    "example_series = pd.Series(example_dict)\n",
    "print(example_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements in the `Series` can now be accessed using the corresponding keys from the `dict` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_dict['b'])\n",
    "print(example_dict['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** You can also use the `index` argument when constructing a `Series` from a `dict`. However, only values with indices included **both** as keys in the `dict` and elements in `index` argument will be kept. This may seem clearer with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have specified 3 indices, 'a', 'b' and 'c'\n",
    "example_dict = {'a': 1, 'b': 33, 'c': 2}\n",
    "\n",
    "print(example_dict)\n",
    "\n",
    "# Now, we choose to use only 'a' and 'c' so only 'a' and 'c'\n",
    "# will be in the Series object\n",
    "print(pd.Series(example_dict, index=['a','c']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `Dataframes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `pandas` `DataFrame` object can be thought of as a collection of aligned `pandas` `Series` objects, where by aligned we mean sharing the same index. A `DataFrame` can be constructed from several `Series` using the `pandas.DataFrame` function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of heights in inches\n",
    "heights = pd.Series({'Pete': 69, 'Mo': 72, 'Katy': 64, 'Alex': 80})\n",
    "# Series of weights in kg\n",
    "weights = pd.Series({'Pete': 60, 'Mo': 80, 'Jay': 55, 'Claire': 70})\n",
    "\n",
    "example_dataframe = pd.DataFrame({'heights': heights, 'weights': weights})\n",
    "\n",
    "# In this notebook we can display a pandas dataframe in a\n",
    "# nice format without printing just by typing the dataframes\n",
    "# name.\n",
    "example_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** The `Series` objects used to create a `DataFrame` do not have to contain the same indices as one another to be combined to make a `DataFrame`. In the above example `Claire` appeared in the `weights` series but not the `heights` series. One of the great features of `Pandas` is that it will automatically fill any missing values in with an `NaN` (such as Claire's height in this example). \n",
    "\n",
    "A `DataFrame` can also be constructed from a `dict` of `dict`s like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of heights in inches\n",
    "heights = {'Pete': 69, 'Mo': 72, 'Katy': 64, 'Alex': 80}\n",
    "# Series of weights in kg\n",
    "weights = {'Pete': 60, 'Mo': 80, 'Jay': 55, 'Claire': 70}\n",
    "\n",
    "example_dataframe = pd.DataFrame({'heights': heights, 'weights': weights})\n",
    "\n",
    "example_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a `DataFrame` can also be constructed from a `numpy` array like so; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataframe = pd.DataFrame(\n",
    "    np.array([[62,66,75],[63,65,71]]))\n",
    "\n",
    "example_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows and columns can be lablled with the `index` and `columns` arguments respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataframe = pd.DataFrame(\n",
    "    np.array([[62,66,75],[63,65,71]]),\n",
    "    columns=['heights', 'weights', 'favourite number'],\n",
    "    index=['Joe', 'Catelyn'])\n",
    "\n",
    "example_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices can be retrieved from a `Series` or `DataFrame` using the `index` attribute. This can be done like so;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_series.index)\n",
    "print(example_dataframe.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column headers from a `DataFrame` can also be retrieved using the `columns` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` datatypes are incredibly useful and efficient when dealing with heterogeneous data. In this section we will give a, by no means comprehenive, selection of useful functions that `pandas` offers. We will use the below `DataFrame` in the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_dataset = pd.read_csv('pandas/toydataset.csv')\n",
    "\n",
    "# Show our dataset\n",
    "example_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean indexing can be used to return subsets of a `DataFrame`. What is particularly nice about this is that in `Pandas` boolean logic is much easier to interpret, due to the use of column names. For example, hopefully, the below line of code should be fairly intuitive;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset[(example_dataset['sex']=='M') & \n",
    "                (example_dataset['birth_year']>1987)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** When using multiple boolean statements always use `()` brackets to make your logic clearer and less susceptible to coding errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above is useful but the notation is a bit clunky - we had to type `example_dataset` three times to do this operation. We can do this in a much cleaner fashion using the `query` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the same operation as in the above section with much cleaner syntax using the `query` function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.query('(sex == \"M\") & (birth_year > 1987)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Warning:** The `query` function may have problems with column names which can't be used as python identifiers (for example column names including a space). This is a common cause of `SyntaxError`'s for users new to `Pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing `NaN`s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting data it may be desirable to first remove `NaN` values/missing data which could interfere with future operations. Rows with `NaN` values in a specified column can be found and removed using the `isna` function like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all subjects for whom we didn't record sex\n",
    "example_dataset[~example_dataset.sex.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually a method that does this in a much neater fashion; the `dropna` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all subjects with missing sex and birthyear information\n",
    "example_dataset.dropna(subset=['birth_year', 'sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sorting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One operation which you may want to do when first looking at data is sort it by a variable of interest. Sorting can be done in `pandas` using the `sort_values` method. For example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_example = example_dataset.sort_values('birth_year')\n",
    "sorted_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a row of the new sorted DataFrame, the `iloc` function can be used to index it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_example.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** We can still select the index `0` of the DataFrame under it's **original ordering** using `loc`. Be careful doing this, however, as it can be very easy to confuse the `loc` and `iloc` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_example.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great strength of the `pandas` package is that, much like `numpy`, it contains several methods for getting summary measures of columns of data. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "example_dataset.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation\n",
    "example_dataset.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of non-missing/non-NaN values\n",
    "example_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function which is useful as a sanity check whenever you are working in `pandas` is the `describe` method, which gives several common summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.describe()\n",
    "\n",
    "# (The percentages in this table are quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our own custom functions to apply to the columns of a `DataFrame` using the `apply` method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a T statistic for the t-test that\n",
    "# the mean=1984.5\n",
    "#\n",
    "# i.e. T = (mean-1985)/(std/sqrt(n))\n",
    "def tstat(series):\n",
    "    \n",
    "    # Get T statistic\n",
    "    tStat = (series.mean()-1984.5)/(series.std()/np.sqrt(series.count()))\n",
    "    \n",
    "    return(tStat)\n",
    "\n",
    "# Return the T statistics... all but birth year should have large \n",
    "# T statistic values as only the birth years had mean 1984.5\n",
    "example_dataset.select_dtypes(np.number).apply(tstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Warning:** When applying custom functions to a `DataFrame` you must ensure you specify which datatypes you want to apply your function to. In the above we used the `select_dtypes` argument to specify that we wanted to apply our function to any column containing entries that are `np.number` objects (i.e. `np.int64`, `np.float32`, etc...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method actually allows us to apply several functions at once. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.select_dtypes(np.number).apply([tstat, \n",
    "                                                'mean', \n",
    "                                                np.prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing by groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely useful tool is the `groupby` tool which groups the data based on categorical variables of the users choosing. This returns an object, can be iterated through like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex, table_for_category in example_dataset.groupby('sex'):\n",
    "    \n",
    "    print(f'Mean age for sex {sex}: {table_for_category.birth_year.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common application of the `groupby` method, however, is combining it with the `apply` function to get summary measures for groups. This is an extremely powerful functionality. For example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.groupby('group').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to group by the intersection of multiple categories like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.groupby(['sex','group']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** If one of the entries in the above is `NaN` then there were no subjects of that sex in that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is even possible to do multiple groupings and multiple summary statistics at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean and median for each (sex, group) combination \n",
    "example_dataset.groupby(['sex','group']).aggregate((np.median, np.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be useful stratify based on a continuous variable. This can be done easily using the `cut` function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the years into groups and take their means\n",
    "example_dataset.groupby(\n",
    "    pd.cut(example_dataset.birth_year, \n",
    "           bins=(1979, 1983, 1986, 1990))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we were interested in the mean `eyelash_length` of every `group` for each `birth_year`. We could do the following;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.groupby(['birth_year','group']).eyelash_length.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are 10 `birth_year`s and 5 `group`s. This means there are potentially up to 50 lines in the above table. In this example, we probably wouldn't worry too much but on real data this could result in us getting very large tables very quickly!\n",
    "\n",
    "One way around this is to use the `unstack` method, which moves one of the groupings in the rows into the columns like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.groupby(\n",
    "                ['birth_year','group']\n",
    "            ).eyelash_length.mean().unstack('group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shorter table is often referred to as \"wide form\" whilst the taller is referred to as \"long form\". \n",
    "\n",
    "There are many reasons you may want to work with both forms of data. For example, it often more convenient to group data using long form tables but often easier to generate plots with wide form tables. Often, the choice of format is also a question of personal preference.\n",
    "\n",
    "You can convert wide form back to long form using the `stack` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack the dataframe\n",
    "unstacked = example_dataset.groupby(\n",
    "                ['birth_year','group']\n",
    "            ).eyelash_length.mean().unstack('group')\n",
    "\n",
    "# Stack the dataframe\n",
    "stacked = unstacked.stack('group')\n",
    "\n",
    "stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to the `groupby`, `stack` and `unstack` approach is to use the functions `pivot_table` and `melt`.\n",
    "\n",
    "`pivot_table` is particularly useful as, not only does it unstack the table, but it also handles grouping at the same time. For example, we could perform the same unstacking operation we did earlier with `groupby` and `unstack` using `pivot_table` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unstacked = example_dataset.pivot_table('eyelash_length', \n",
    "                                      'birth_year', \n",
    "                                      'group')\n",
    "unstacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can unstack the table we just made with the `melt` method. This is a little more fiddly, however, as in the previous example, using `pivot_table` changed the `birth_year` variable from a column in the table to the row index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change `birth_year` back to a column\n",
    "unstacked['birth_year'] = unstacked.index\n",
    "\n",
    "# Unstack our table\n",
    "stacked = pd.melt(unstacked, id_vars=['birth_year'], value_vars=['a','b','c','d','e'],\n",
    "         var_name='group', value_name='eyelash_length')\n",
    "\n",
    "# Our table will not be in the original order but we can sort that\n",
    "# After sorting the indices will be out of order so we reset those.\n",
    "stacked.sort_values(['birth_year', 'group']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** Our table is now longer. This is because we have included `NaN`/missing values we didn't include in our original table. We could remove these easily though, for example, with the `dropna` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading/Writing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final footnote on `pandas` is that it is extremely fast at reading and writing files.\n",
    "\n",
    "`pandas` provides a wide range of tools for loading from text files, binary files, and SQL databases. The most commonly used function for this purpose is the `pandas.read_{format}` function. For example, to read `csv` files we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_from_csv = pd.read_csv('pandas/example_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write to a csv using the `pandas.to_csv`  like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.random.randn(1000,1000))\n",
    "\n",
    "X.to_csv('pandas/example_data_we_made.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the fastest file reading tools in Python and worth knowing about! For example, compare the time `pandas` took to read the example `csv` to the time `numpy` takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Let's time pandas\n",
    "t1 = time.time()\n",
    "array_from_csv = pd.read_csv('pandas/example_data.csv').values\n",
    "t2 = time.time()\n",
    "print('pandas took ', t2-t1, ' seconds to read the csv file.')\n",
    "\n",
    "# Now time numpy\n",
    "t1 = time.time()\n",
    "array_from_csv = np.loadtxt('pandas/example_data.csv', delimiter=',')\n",
    "t2 = time.time()\n",
    "print('numpy took ', t2-t1, ' seconds to read the csv file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` is an extremely powerful tool. We have barely scratched the surface on it's capabilities here. To learn more, please see the official [`pandas` documentation here](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Read in the file `exercise_dataset.csv` as a pandas DataFrame. We are not interested in the `subject_id` variable so remove this from the DataFrame. *Hint: The [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function may be useful for this*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** After collecting this data, you have found out that some, an extremely small minority in fact, of the subjects thought it would be funny to deliberately fill out the `favourite_animal` field in your survey with an object that is not an animal! Fearing these subjects may have filled out other values incorrectly, you wish to remove them from the DataFrame immediately.\n",
    "\n",
    "Investigate the `.value_counts()` function in the [`Pandas` documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html). What does this function do? Use the function to find the rows of the DataFrame with non-animal responses in the `favourite_animal` column and remove the rows from the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Your research is interested in the relationship between the size of the big toe on the left foot of each subject (`size_of_left_toe`) and the subjects heartrates (`heartrate`). Because of this, you have decided that you don't want to include any subjects which have any `NaN` values in either the `size_of_left_toe` or `heartrate` columns.\n",
    "\n",
    "Remove all rows of the DataFrame which include `NaN` values in either the `size_of_left_toe` or `heartrate` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Write a function which takes in a numpy array and a percentage $k$ and returns the k% quantile of the array. Now apply it to the columns of your DataFrame to get the 5% and 95% quantiles of each column. Check your results against the inbuilt pandas `quantile` function. *Note: You do not have to worry about interpolating values and you can use the `numpy sort` attribute for this question. However, **no other numpy functions** should be used!*\n",
    "\n",
    "\n",
    "Use boolean indexing, and either your own function or the `quantile` function, to return a DataFrame containing only the subjects who were in the top 5\\% quantile for `heartrate` and the bottom 5\\% quantile for `size_of_left_toe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** The range of the `age` column in the DataFrame is from 10 to 59 years old. Perform the following operations on the DataFrame;\n",
    "\n",
    " - Use the `cut` function to seperate subjects into 5 age groups, each of ten years in length. \n",
    " \n",
    "\n",
    " - For each age range, obtain the mean `size_of_left_toe` and mean `heartrate` for male (`M`) subject and female (`F`) subjects seperately. Save a Dataframe containing these values as `mean_age_sex`. *Hint: Consider using the `groupby` function.*\n",
    " \n",
    "\n",
    " - For each age range, obtain the mean `size_of_left_toe` and mean `heartrate` for each of the groups listed in the `group` column (i.e. `a`, `b`, `c`, `d` and `e`). Save this in your workspace as `mean_age_group`. \n",
    " \n",
    "\n",
    " - Stack `mean_age_sex` and `mean_age_group` on top of one another using the [`concat` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html). \n",
    " \n",
    "\n",
    "In practice, this is probably not a good way to lay out this data. List some reasons why you may not wish to lay out the data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
